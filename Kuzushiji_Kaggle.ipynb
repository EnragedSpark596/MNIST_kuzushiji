{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.kaggle.com/anokas/kuzushiji\n",
    "\n",
    "Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in\n",
    "the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one\n",
    "character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.\n",
    "\n",
    "kmnist-[train/test]-[images/labels].npz: These files contain the Kuzushiji-MNIST as compressed numpy arrays,\n",
    "and can be read with: arr = np.load(filename)['arr_0']. We recommend using these files to load the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianda\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:/Users/ianda/Desktop/Dropbox (NSF CORE)/Coding Projects/Python/Udemy_MachineLearning/Kuzushiji/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.load.html\n",
    "# https://stackoverflow.com/questions/44289302/how-do-i-view-data-object-contents-within-an-npz-file\n",
    "# https://stackoverflow.com/questions/32682928/loading-arrays-from-npz-files-in-pythhon\n",
    "# https://stackoverflow.com/questions/48429408/open-and-view-npz-file-in-python\n",
    "\n",
    "train_imgs = np.load(filepath+'k49-train-imgs.npz')['arr_0']\n",
    "test_imgs = np.load(filepath+'k49-test-imgs.npz')['arr_0']\n",
    "train_labels = np.load(filepath+'k49-train-labels.npz')['arr_0']\n",
    "test_labels = np.load(filepath+'k49-test-labels.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232365"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_len = len(train_imgs)\n",
    "test_imgs_len = len(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_x = train_imgs.reshape(train_imgs_len, 784) # 60000 items, flattened from 28x28 to linear 784\n",
    "test_imgs_x = test_imgs.reshape(test_imgs_len, 784)\n",
    "\n",
    "# normalize data (starts as 0-255 integer)\n",
    "# step 1 converts them to float\n",
    "train_imgs_x = train_imgs_x.astype('float32')\n",
    "test_imgs_x = test_imgs_x.astype('float32')\n",
    "\n",
    "#step 2 converts them to range 0-1\n",
    "train_imgs_x /= 255\n",
    "test_imgs_x /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(np.unique(train_labels))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_y = tensorflow.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_y = tensorflow.keras.utils.to_categorical(test_labels, num_classes) # converts to one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels_y[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels_y[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_imgs_x[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (232365, 28, 28)\n",
      "train_label: (232365,)\n",
      "test_data: (38547, 28, 28)\n",
      "test_label: (38547,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_images: {}\".format(train_imgs.shape))\n",
    "print(\"train_label: {}\".format(train_labels.shape))\n",
    "print(\"test_data: {}\".format(test_imgs.shape))\n",
    "print(\"test_label: {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # means you can add layers one at a time\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,))) # dense first hidden layer of 512 neurons\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 49)                25137     \n",
      "=================================================================\n",
      "Total params: 427,057\n",
      "Trainable params: 427,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 232365 samples, validate on 38547 samples\n",
      "Epoch 1/10\n",
      " - 33s - loss: 0.7657 - acc: 0.8033 - val_loss: 0.8234 - val_acc: 0.7946\n",
      "Epoch 2/10\n",
      " - 29s - loss: 0.4071 - acc: 0.8937 - val_loss: 0.7334 - val_acc: 0.8259\n",
      "Epoch 3/10\n",
      " - 27s - loss: 0.3267 - acc: 0.9144 - val_loss: 0.7127 - val_acc: 0.8379\n",
      "Epoch 4/10\n",
      " - 27s - loss: 0.2799 - acc: 0.9270 - val_loss: 0.7297 - val_acc: 0.8418\n",
      "Epoch 5/10\n",
      " - 28s - loss: 0.2491 - acc: 0.9354 - val_loss: 0.7538 - val_acc: 0.8419\n",
      "Epoch 6/10\n",
      " - 29s - loss: 0.2222 - acc: 0.9429 - val_loss: 0.7967 - val_acc: 0.8439\n",
      "Epoch 7/10\n",
      " - 26s - loss: 0.2018 - acc: 0.9480 - val_loss: 0.8334 - val_acc: 0.8414\n",
      "Epoch 8/10\n",
      " - 24s - loss: 0.1840 - acc: 0.9524 - val_loss: 0.8706 - val_acc: 0.8440\n",
      "Epoch 9/10\n",
      " - 24s - loss: 0.1690 - acc: 0.9565 - val_loss: 0.8924 - val_acc: 0.8442\n",
      "Epoch 10/10\n",
      " - 27s - loss: 0.1556 - acc: 0.9598 - val_loss: 0.9522 - val_acc: 0.8437\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_imgs_x, train_labels_y,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_imgs_x, test_labels_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_imgs_2Dx = train_imgs.reshape(train_imgs.shape[0], 1, 28, 28)\n",
    "    test_imgs_2Dx = test_imgs.reshape(test_imgs.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_imgs_2Dx = train_imgs.reshape(train_imgs.shape[0], 28, 28, 1)\n",
    "    test_imgs_2Dx = test_imgs.reshape(test_imgs.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_imgs_2Dx = train_imgs_2Dx.astype('float32') # data originally 8-bit bytes (256 bits?)?\n",
    "test_imgs_2Dx = test_imgs_2Dx.astype('float32')\n",
    "train_imgs_2Dx /= 255 # to transform images data to a value between 0 and 1\n",
    "test_imgs_2Dx /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_2Dy = tensorflow.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_2Dy = tensorflow.keras.utils.to_categorical(test_labels, num_classes) # converts to one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # allows model to be built in layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), # 32 regional fields that model will usw to sample the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape defined in line 6/10\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pixel found (removes blurry ones to save space?)\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 1,204,913\n",
      "Trainable params: 1,204,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 232365 samples, validate on 38547 samples\n",
      "Epoch 1/10\n",
      " - 526s - loss: 0.9914 - acc: 0.7350 - val_loss: 0.6483 - val_acc: 0.8260\n",
      "Epoch 2/10\n",
      " - 534s - loss: 0.5612 - acc: 0.8440 - val_loss: 0.4840 - val_acc: 0.8732\n",
      "Epoch 3/10\n",
      " - 313s - loss: 0.4520 - acc: 0.8729 - val_loss: 0.4118 - val_acc: 0.8917\n",
      "Epoch 4/10\n",
      " - 313s - loss: 0.3866 - acc: 0.8902 - val_loss: 0.3886 - val_acc: 0.8984\n",
      "Epoch 5/10\n",
      " - 312s - loss: 0.3445 - acc: 0.9016 - val_loss: 0.3594 - val_acc: 0.9057\n",
      "Epoch 6/10\n",
      " - 312s - loss: 0.3151 - acc: 0.9094 - val_loss: 0.3419 - val_acc: 0.9124\n",
      "Epoch 7/10\n",
      " - 302s - loss: 0.2904 - acc: 0.9146 - val_loss: 0.3340 - val_acc: 0.9136\n",
      "Epoch 8/10\n",
      " - 299s - loss: 0.2755 - acc: 0.9190 - val_loss: 0.3210 - val_acc: 0.9173\n",
      "Epoch 9/10\n",
      " - 299s - loss: 0.2566 - acc: 0.9240 - val_loss: 0.3212 - val_acc: 0.9186\n",
      "Epoch 10/10\n",
      " - 299s - loss: 0.2446 - acc: 0.9274 - val_loss: 0.3240 - val_acc: 0.9184\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_imgs_2Dx, train_labels_2Dy,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2, # right for jupyter\n",
    "                    validation_data=(test_imgs_2Dx, test_labels_2Dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D with different tuning - more like malaria kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # allows model to be built in layers\n",
    "#input layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), # 32 regional fields that model will usw to sample the image\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape defined in line 6/10\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 16)          2064      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               18560     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 53,985\n",
      "Trainable params: 53,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 232365 samples, validate on 38547 samples\n",
      "Epoch 1/10\n",
      " - 292s - loss: 1.2347 - acc: 0.6627 - val_loss: 0.8207 - val_acc: 0.7797\n",
      "Epoch 2/10\n",
      " - 294s - loss: 0.6766 - acc: 0.8082 - val_loss: 0.6661 - val_acc: 0.8174\n",
      "Epoch 3/10\n",
      " - 294s - loss: 0.5725 - acc: 0.8357 - val_loss: 0.5345 - val_acc: 0.8543\n",
      "Epoch 4/10\n",
      " - 295s - loss: 0.5225 - acc: 0.8500 - val_loss: 0.4900 - val_acc: 0.8652\n",
      "Epoch 5/10\n",
      " - 295s - loss: 0.4877 - acc: 0.8603 - val_loss: 0.4607 - val_acc: 0.8735\n",
      "Epoch 6/10\n",
      " - 293s - loss: 0.4666 - acc: 0.8660 - val_loss: 0.4386 - val_acc: 0.8810\n",
      "Epoch 7/10\n",
      " - 295s - loss: 0.4446 - acc: 0.8719 - val_loss: 0.4189 - val_acc: 0.8875\n",
      "Epoch 8/10\n",
      " - 293s - loss: 0.4296 - acc: 0.8761 - val_loss: 0.4050 - val_acc: 0.8915\n",
      "Epoch 9/10\n",
      " - 293s - loss: 0.4183 - acc: 0.8792 - val_loss: 0.3991 - val_acc: 0.8929\n",
      "Epoch 10/10\n",
      " - 304s - loss: 0.4069 - acc: 0.8826 - val_loss: 0.3896 - val_acc: 0.8950\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_imgs_2Dx, train_labels_2Dy,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2, # right for jupyter\n",
    "                    validation_data=(test_imgs_2Dx, test_labels_2Dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
